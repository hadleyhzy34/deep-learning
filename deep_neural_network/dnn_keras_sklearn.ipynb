{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "ANN\n",
    "TLU(threshold logic unit), linear threshold unit(LTU)\n",
    "activation function:\n",
    "1.sigmoid function(y=1/(1+e^(-x)))\n",
    "2.relu(y=max(0,x))\n",
    "3.step function:\n",
    "3.1 heaviside(z) = (z>=0)?1:0\n",
    "\n",
    "\n",
    "Perceptron\n",
    "the decision boundary of each output neuron is linear, perceptrons are incapable of learning complex patterns. \n",
    "Perceptron convergence theorem:\n",
    "if the training instances are linearly separable, this algorithm would converge to a solution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:,(2,3)]\n",
    "y = (iris.target == 0).astype(np.int) #if iris setosa, output 1, otherwise 0\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X,y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "print(y_pred)"
   ]
  },
  {
   "source": [
    "note that perceptron make predictions based on a hard threshold, hwile logistic regression classifier outpu a class probability.\n",
    "XOR cannot be solved by using perceptrons or any other linear classification model including logistic regression model. However, a simple transformation to the feature space, like x1 * x2 this kind of nonlinear feature allows logistic regression to learn a decision.\n",
    "\n",
    "by stacking multiple perceptrons, which is called MLP(multiple perceptron) could solve this problem.\n",
    "\n",
    "check book page288 to see how it could solve XOR problem"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "MLP:\n",
    "input layer\n",
    "hidden layer\n",
    "output layer\n",
    "\n",
    "1. the layers that close to the input layer are usually called the lower layers, and the ones close to the outputs are usually called the upper layers\n",
    "2. every layers except otuput layers includes a bias neuron and is fully connected to the next layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch Gradient descent: batch size = size of training set\n",
    "stochastic gradient descent. batch size =1\n",
    "mini-batch gradient descent. 1<batch size<size of training set"
   ]
  },
  {
   "source": [
    "it is important to initialize all the hidden layers' connectin weights randomly, or else training will fail. For example, if you initialize all weights and biases to zero, then all neurons in a given layer will be perfectly identical, and thus backpropagation will affect them in exactly the same way, so they will remain identical. In other words, despite having hundres of neurons per layer, your model will act as if it had one one neuron per layer: it won't be smart. If instead you randomly initialize the weights, you break the symmetry and allow backprograpation to train a diverse team of neurons."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "change perceptron step function to MLP sigmoid function\n",
    "\n",
    "* step function\n",
    "* sigmoid function\n",
    "* hyperbolic tangent function:\n",
    "tanh(z) = sinh(x)/cosh(x)   #https://www.mathworks.com/help/matlab/ref/tanh.html\n",
    "that range tends to make each layer's output more or less centered around 0 at the beginning of training, which oftern helps speed up convergence\n",
    "* ReLU(z) = max(0,z)\n",
    "slope changes abruptly and its derivative is 0 for z<0, in pratice it has become the default since it has the advantage of being fast to compute\n",
    "\n",
    "* softplus activation function softplus(z)=log(1+exp(z)) which is smooth variant of ReLU, it is close to 0 when z is negative, and close to z when z is positive.\n",
    "\n",
    "## the key idea why we need activation functions:\n",
    "if chain several linear transformations, it is still linear transformation. so there's no nonlinearity between layers, even deep stack of layers is still quivalent to a single layer."
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applications:\n",
    "1. regression MLPs\n",
    "1.1 output neurons\n",
    "it depends on number of values need to be predicted\n",
    "* single value->single output neurons\n",
    "* multiple values->one output neuron per output dimension\n",
    "1.2 for output neurons, normally do not use any activation function, but alternatively choose different activation functions depending on output range of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss function/evaluation metric to use \n",
    "1.during training is typically the mean sqaured error\n",
    "2.if you have alot of outliers in the training set, you may prefer to use the mean absolute error instead.\n",
    "\n",
    "the average difference observed in the predicted and actual values across the whole test set.\n",
    "\n",
    "page 293 not correct\n",
    "\n",
    "Taking the square root of the average squared errors has some interesting implications for RMSE. Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE should be more useful when large errors are particularly undesirable. The three tables below show examples where MAE is steady and RMSE increases as the variance associated with the frequency distribution of error magnitudes also increases.\n",
    "\n",
    "3.RMSE\n",
    "\n",
    "4.huber loss\n",
    "\n",
    "* outlier\n",
    "\n",
    "An outlier is an object that deviates significantly from the rest of the objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary:\n",
    "input neurons:one per input features\n",
    "hidden layers:depends, typically 1 to 5\n",
    "neurons per hidden layer:depends on the problem, typically 10 to 100\n",
    "output neurons:1 perdiction dimension\n",
    "hidden activation:Relu\n",
    "output activation:none or depends\n",
    "loss: mse, rmse,mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification MLPs\n",
    "\n",
    "binary classification:\n",
    "output layer activation:sigmoid\n",
    "loss function:cross entropy(log loss)\n",
    "\n",
    "multilabel binary classification:\n",
    "output neurons:1 per label\n",
    "loss function: cross entropy\n",
    "\n",
    "multiclss classification:\n",
    "output neurons: 1 per class\n",
    "loss function:cross entropy"
   ]
  }
 ]
}