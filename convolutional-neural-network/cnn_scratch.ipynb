{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3x3:\n",
    "  def __init__(self, num_filters):\n",
    "    self.num_filters = num_filters\n",
    "\n",
    "    # filters is a 3d array with dimensions (num_filters, 3, 3)\n",
    "    # We divide by 9 to reduce the variance of our initial values\n",
    "    self.filters = np.random.randn(num_filters, 3, 3) / 9\n",
    "\n",
    "  def iterate_regions(self, image):\n",
    "    '''\n",
    "    Generates all possible 3x3 image regions using valid padding.\n",
    "    - image is a 2d numpy array\n",
    "    '''\n",
    "    h, w = image.shape\n",
    "\n",
    "    for i in range(h - 2):\n",
    "      for j in range(w - 2):\n",
    "        im_region = image[i:(i + 3), j:(j + 3)]\n",
    "        yield im_region, i, j\n",
    "\n",
    "  def forward(self, input):\n",
    "    '''\n",
    "    Performs a forward pass of the conv layer using the given input.\n",
    "    Returns a 3d numpy array with dimensions (h, w, num_filters).\n",
    "    - input is a 2d numpy array\n",
    "    '''\n",
    "    h, w = input.shape\n",
    "    output = np.zeros((h - 2, w - 2, self.num_filters))\n",
    "\n",
    "    for im_region, i, j in self.iterate_regions(input):\n",
    "        # print(im_region * self.filters)\n",
    "        # if i == 1 and j == 1:\n",
    "            # print(im_region)\n",
    "            # print(self.filters)\n",
    "            # print(im_region*self.filters)\n",
    "        output[i, j] = np.sum(im_region * self.filters, axis=(1, 2))\n",
    "        # print(output[i,j])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets, layers, models\n",
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "((train_data, train_labels),\n",
    " (test_data, test_labels)) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[[-0.16579106  0.04157581  0.18261762]\n  [ 0.02589571  0.04870474 -0.04001759]\n  [ 0.03043945 -0.00339932  0.06048421]]\n\n [[-0.05520445 -0.00963506 -0.07474203]\n  [-0.06806303  0.01596686  0.03544576]\n  [-0.0733058   0.08596688  0.05192666]]\n\n [[ 0.18306948  0.24292954  0.00098075]\n  [ 0.21271711  0.13813746  0.04442928]\n  [-0.05206455 -0.05296049  0.02648289]]\n\n [[ 0.02287851 -0.12688691 -0.1565667 ]\n  [ 0.16806256  0.09273228  0.02998046]\n  [-0.04214815  0.16788204 -0.1187797 ]]\n\n [[ 0.12209162 -0.13940288 -0.08098008]\n  [-0.12857576  0.07461258 -0.00906119]\n  [-0.09104775 -0.06795708 -0.14713846]]\n\n [[-0.01371948 -0.08388236  0.02789888]\n  [ 0.20024302  0.08422004 -0.12890142]\n  [ 0.01407955 -0.06800342  0.03944906]]\n\n [[-0.21172107 -0.01246743  0.17083178]\n  [ 0.01787524 -0.02465381 -0.05946606]\n  [-0.2502538  -0.13698581  0.17217904]]\n\n [[-0.1046654   0.04590083 -0.18127095]\n  [-0.09283195  0.02560757  0.14189243]\n  [ 0.36817788 -0.07548666 -0.05800406]]]\n(26, 26, 8)\n"
     ]
    }
   ],
   "source": [
    "conv = Conv3x3(8)\n",
    "output = conv.forward(train_data[0])\n",
    "print(conv.filters)\n",
    "print(output.shape)"
   ]
  },
  {
   "source": [
    "## implement pooling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2:\n",
    "    def iterate_regions(self,image):\n",
    "       h,w,_ = image.shape\n",
    "       new_h = h//2\n",
    "       new_w = w//2\n",
    "\n",
    "       for i in range(0,w,2):\n",
    "           for j in range(0,h,2):\n",
    "               im_region = image[i:(i+2),j:(j+2)]\n",
    "               yield im_region, i//2 ,j//2\n",
    "\n",
    "    def forward(self, input):\n",
    "        h,w,num_filters = input.shape\n",
    "        output = np.zeros((h//2,w//2,num_filters))\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(input):\n",
    "            output[i,j] = np.amax(im_region, axis=(0,1))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(13, 13, 8)\n"
     ]
    }
   ],
   "source": [
    "pool = MaxPool2()\n",
    "output = pool.forward(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "source": [
    "## cross-entropy loss and softmax"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "What softmax really does is help us quantify how sure we are of our prediction, which is useful when training and evaluating our CNN. More specifically, using softmax lets us use cross-entropy loss, which takes into account how sure we are of each prediction. Here’s how we calculate cross-entropy loss:\n",
    "\n",
    "L = -ln(p_c)\n",
    "\n",
    "where c is the correct class (in our case, the correct digit), p_c is the predicted probability for class c, and ln is the natural log. As always, a lower loss is better. For example, in the best case, we’d have\n",
    "\n",
    "p_c = 1, L = -ln(1) = 0\n",
    "\n",
    "In a more realistic case, we might have\n",
    "\n",
    "p_c = 0.8, L = -ln(0.8) = 0.223"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "  # A standard fully-connected layer with softmax activation.\n",
    "  # nodes number of labels/classes, input_len: number of elements for output of pooling layer\n",
    "  def __init__(self, input_len, nodes):\n",
    "    # We divide by input_len to reduce the variance of our initial values\n",
    "    self.weights = np.random.randn(input_len, nodes) / input_len\n",
    "    self.biases = np.zeros(nodes)\n",
    "\n",
    "  def forward(self, input):\n",
    "    '''\n",
    "    Performs a forward pass of the softmax layer using the given input.\n",
    "    Returns a 1d numpy array containing the respective probability values.\n",
    "    - input can be any array with any dimensions.\n",
    "    '''\n",
    "    input = input.flatten()\n",
    "\n",
    "    input_len, nodes = self.weights.shape\n",
    "\n",
    "    total = np.dot(input,self.weights) + self.biases\n",
    "    # print(input.shape,self.weights.shape,total.shape)\n",
    "\n",
    "    total = np.exp(total)\n",
    "    return total / np.sum(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.02143594 0.00228761 0.25976688 0.26191097 0.18616113 0.04974702\n 0.01968595 0.02979915 0.0050981  0.16410727]\n"
     ]
    }
   ],
   "source": [
    "softmax = Softmax(13*13*8,10)\n",
    "out = softmax.forward(output)\n",
    "print(out)"
   ]
  },
  {
   "source": [
    "## Model Initialization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MNIST CNN initialized!\n",
      "[Step 100] Past 100 steps: Average Loss 2.303 | Accuracy: 14%\n",
      "[Step 200] Past 100 steps: Average Loss 2.303 | Accuracy: 15%\n",
      "[Step 300] Past 100 steps: Average Loss 2.304 | Accuracy: 10%\n",
      "[Step 400] Past 100 steps: Average Loss 2.303 | Accuracy: 10%\n",
      "[Step 500] Past 100 steps: Average Loss 2.303 | Accuracy: 8%\n",
      "[Step 600] Past 100 steps: Average Loss 2.302 | Accuracy: 11%\n",
      "[Step 700] Past 100 steps: Average Loss 2.303 | Accuracy: 6%\n",
      "[Step 800] Past 100 steps: Average Loss 2.302 | Accuracy: 16%\n",
      "[Step 900] Past 100 steps: Average Loss 2.303 | Accuracy: 10%\n",
      "[Step 1000] Past 100 steps: Average Loss 2.303 | Accuracy: 12%\n"
     ]
    }
   ],
   "source": [
    "import mnist\n",
    "# We only use the first 1k testing examples (out of 10k total)\n",
    "# in the interest of time. Feel free to change this if you want.\n",
    "test_images = mnist.test_images()[:1000]\n",
    "test_labels = mnist.test_labels()[:1000]\n",
    "\n",
    "conv = Conv3x3(8)                  # 28x28x1 -> 26x26x8\n",
    "pool = MaxPool2()                  # 26x26x8 -> 13x13x8\n",
    "softmax = Softmax(13 * 13 * 8, 10) # 13x13x8 -> 10\n",
    "\n",
    "def forward(image, label):\n",
    "  '''\n",
    "  Completes a forward pass of the CNN and calculates the accuracy and\n",
    "  cross-entropy loss.\n",
    "  - image is a 2d numpy array\n",
    "  - label is a digit\n",
    "  '''\n",
    "  # We transform the image from [0, 255] to [-0.5, 0.5] to make it easier\n",
    "  # to work with. This is standard practice.\n",
    "  out = conv.forward((image / 255) - 0.5)\n",
    "  out = pool.forward(out)\n",
    "  out = softmax.forward(out)\n",
    "\n",
    "  # Calculate cross-entropy loss and accuracy. np.log() is the natural log.\n",
    "  loss = -np.log(out[label])\n",
    "  acc = 1 if np.argmax(out) == label else 0\n",
    "\n",
    "  return out, loss, acc\n",
    "\n",
    "print('MNIST CNN initialized!')\n",
    "\n",
    "loss = 0\n",
    "num_correct = 0\n",
    "for i, (im, label) in enumerate(zip(test_images, test_labels)):\n",
    "  # Do a forward pass.\n",
    "  _, l, acc = forward(im, label)\n",
    "  loss += l\n",
    "  num_correct += acc\n",
    "\n",
    "  # Print stats every 100 steps.\n",
    "  if i % 100 == 99:\n",
    "    print(\n",
    "      '[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%' %\n",
    "      (i + 1, loss / 100, num_correct)\n",
    "    )\n",
    "    loss = 0\n",
    "    num_correct = 0"
   ]
  },
  {
   "source": [
    "## backprop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### input to the softmax layer's backward phase"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "#calculate initial gradient\n",
    "nodes = 10\n",
    "gradient = np.zeros(nodes)\n",
    "gradient[label] = -1 / out[label]\n",
    "print(label)"
   ]
  },
  {
   "source": [
    "### prepare caching during the forward phase"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "  def __init__(self, input_len, nodes):\n",
    "    self.weights = np.random.randn(input_len, nodes) / input_len\n",
    "    self.biases = np.zeros(nodes)\n",
    "\n",
    "  def forward(self, input):\n",
    "    self.last_input_shape = input.shape\n",
    "    input = input.flatten()\n",
    "    self.last_input = input\n",
    "\n",
    "    input_len, nodes = self.weights.shape\n",
    "\n",
    "    total = np.dot(input,self.weights) + self.biases\n",
    "    self.last_total = total\n",
    "\n",
    "    total = np.exp(total)\n",
    "    return total / np.sum(total)\n",
    "  def backprop(self, d_L_d_out, learn_rate):\n",
    "    '''\n",
    "    Performs a backward pass of the softmax layer.\n",
    "    Returns the loss gradient for this layer's inputs.\n",
    "    - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "    - learn_rate is a float    '''\n",
    "    # We know only 1 element of d_L_d_out will be nonzero\n",
    "    for i, gradient in enumerate(d_L_d_out):\n",
    "      if gradient == 0:\n",
    "        continue\n",
    "\n",
    "      # e^totals\n",
    "      t_exp = np.exp(self.last_totals)\n",
    "\n",
    "      # Sum of all e^totals\n",
    "      S = np.sum(t_exp)\n",
    "\n",
    "      # Gradients of out[i] against totals\n",
    "      d_out_d_t = -t_exp[i] * t_exp / (S ** 2)\n",
    "      d_out_d_t[i] = t_exp[i] * (S - t_exp[i]) / (S ** 2)\n",
    "\n",
    "      # Gradients of totals against weights/biases/input\n",
    "      d_t_d_w = self.last_input\n",
    "      d_t_d_b = 1\n",
    "      d_t_d_inputs = self.weights\n",
    "\n",
    "      # Gradients of loss against totals\n",
    "      d_L_d_t = gradient * d_out_d_t\n",
    "\n",
    "      # Gradients of loss against weights/biases/input\n",
    "      d_L_d_w = d_t_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]\n",
    "      d_L_d_b = d_L_d_t * d_t_d_b\n",
    "      d_L_d_inputs = d_t_d_inputs @ d_L_d_t\n",
    "\n",
    "      # Update weights / biases\n",
    "      self.weights -= learn_rate * d_L_d_w\n",
    "      self.biases -= learn_rate * d_L_d_b\n",
    "      return d_L_d_inputs.reshape(self.last_input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}